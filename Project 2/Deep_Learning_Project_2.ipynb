{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVz7XJnlBSAFvA7jqtbnJa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChrisKantor/Deep-Learning/blob/main/Project%202/Deep_Learning_Project_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Deep learning model that builds on the one made in HW3 for determining an infants pain level using vital signs\n",
        "#This improved model uses LSTM to better understand the sequential readings\n",
        "\n",
        "\n",
        "\n",
        "#Run these commands - NECCESARY to use the GPU\n",
        "# export CUDNN_PATH=$(dirname $(python -c \"import nvidia.cudnn;print(nvidia.cudnn.__file__)\"))\n",
        "# export LD_LIBRARY_PATH=${CUDNN_PATH}/lib\n",
        "\n",
        "#To connect to local runtime: jupyter lab --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888 --NotebookApp.port_retries=0"
      ],
      "metadata": {
        "id": "-gD_j6_iqrxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aRuH-vPap5nA"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import PIL\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Some hyperparameter setup\n",
        "batch_size = 64           #set batch size to 64\n",
        "epochs = 20               #set num epochs to 20\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "# dir = 'Deep Learning/Project 2/'\n",
        "# data_dir = \"data/\"\n",
        "\n",
        "\n",
        "## Mount Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "data_dir = '/content/drive/MyDrive/Colab Notebooks/Deep Learning/Project 2/data/'\n",
        "\n",
        "\n",
        "import os\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z45cpkBIsMQ-",
        "outputId": "fa8535be-deea-486a-a32f-91b1adc1c1d5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n",
            "Mounted at /content/drive\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "CrrDoReswut8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NEEDS FURTHER CLARIFICATION:\n",
        "#If a csv file sequence has an invalid row, do we just remove it or is the whole sequence invalid?\n",
        "#Can we mix the csv sequences together to get a fixed length input sequence? or should each seperate csv file be treated as its own sequence?\n",
        "#Are we predicting if the baby is in pain during the next timestep?\n",
        "\n",
        "\n",
        "\n",
        "#lots to do here - roughly follow this tutorial for general RNN processing: https://www.tensorflow.org/text/tutorials/text_generation\n",
        "\n",
        "#need sequences of data to use as input. Each csv file is 1 sequence, and each of them could potentially have missing/invalid data in them. 602 total csv files\n",
        "#Each csv file has 30 entries, but due to missing/invalid data, our cleaned sequences could have any number of entries\n",
        "#After cleaning the csv files, our dataset will be compromised of a bunch of variable length sequences\n",
        "#we will split our dataset up into training/validation sets on a sequence level\n",
        "\n",
        "#the inputs at a specific timestep will be the 3 labels in the csv files (Heart Rate, Respiratory Rate, O2 level) and the output will be (either if the baby is in pain OR if the baby is in pain during the NEXT step)? -Need clarification here"
      ],
      "metadata": {
        "id": "5wsgp_xGwyhP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d51d25e-5753-4203-9861-4d940fe4f6af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Heart Rate  Respiratory Rate  O2 Level  Pain\n",
            "0          128                49        98     0\n",
            "1          122                47        98     0\n",
            "2          120                92        97     0\n",
            "3          119                90        97     0\n",
            "4          119                50        96     0\n",
            "5          121                47        96     0\n",
            "6          125                47        96     0\n",
            "7          125                47        96     0\n",
            "8          129                47        96     0\n",
            "9          129                47        96     0\n",
            "10         131                47        96     0\n",
            "11         131                46        96     0\n",
            "12         131                48        96     0\n",
            "13         128                47        96     0\n",
            "14         128                46        97     0\n",
            "15         126                48        97     0\n",
            "16         126                38        97     0\n",
            "17         124                38        98     0\n",
            "18         121                38        98     0\n",
            "19         122                38        98     0\n",
            "20         122                30        98     0\n",
            "21         126                30        98     0\n",
            "22         128                31        98     0\n",
            "23         129                31        99     0\n",
            "24         129                31        99     0\n",
            "25         127                33        99     0\n",
            "26         125                33        99     0\n",
            "27         125                40        99     0\n",
            "28         122                56        98     0\n",
            "29         121                99        98     0\n",
            "30         120                61        97     0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#store a list of dataframes representing the data from the csv files we read in\n",
        "#after reading in all of the csv files as dataframes, iterate through them, cleaning up the data and normalizing the inputs\n",
        "\n",
        "\n",
        "#Should we normalize on a dataframe level or a dataset level?\n",
        "\n",
        "\n",
        "data = []\n",
        "\n",
        "\n",
        "#use the os library to iterate through all files in the current path\n",
        "for (root,dirs,files) in os.walk(data_dir, topdown=True):\n",
        "  #open each CSV file, we don't want to open any other type of file\n",
        "  for f in files:\n",
        "    if f.endswith(\".csv\"):\n",
        "      with open(os.path.join(root, f), mode ='r') as csvFile:\n",
        "\n",
        "        #read in the csv file\n",
        "        csvData = csv.reader(csvFile)\n",
        "\n",
        "        #store the data temporarily before we turn it into a pandas dataframe\n",
        "        temp = []\n",
        "\n",
        "\n",
        "        #check if each row is valid\n",
        "        #one way to tell if the data is invalid is if the label is NOT [0, 1, 2]. This means there was something wrong with one of the sensors at the time of capture\n",
        "        #another way is to check if the heart rate, respiratory rate, or o2 level is too low or too high\n",
        "        #for heart Rate, we want a range of 40 - 200\n",
        "        #for Respiratory Rate, we want a range of 15 - 90\n",
        "        #for o2 level, we want a rate of 80 - 100\n",
        "\n",
        "        for row in csvData:\n",
        "          try:\n",
        "            if row[4] in \"012\" and (int(row[1]) >= 40 and int(row[1]) <= 200) and (int(row[2]) >= 15 and int(row[2]) <= 90) and (int(row[3]) >= 80 and int(row[3]) <= 100):   #data is valid, so add it to the main csv\n",
        "              #row[0] just stores the row # in the csv file, so it is not needed in the training dataset\n",
        "              temp.append([int(row[1]), int(row[2]), int(row[3]), int(row[4])])\n",
        "\n",
        "          #Catches errors that occur when an element cannot be parsed into an integer. In this case the row is invalid so we can skip it\n",
        "          except ValueError:\n",
        "            continue\n",
        "\n",
        "        #if this valid and cleaned sequence is NOT empty, add it to our dataset\n",
        "        if len(temp) > 0:\n",
        "          df = pd.DataFrame(temp, columns = ['Heart Rate', 'Respiratory Rate', 'O2 Level', 'Label'])\n",
        "          data.append(df)\n",
        "\n",
        "print(len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0brwvTJ5rkZ",
        "outputId": "bfd9642b-cb70-432d-8261-5bebfcde688c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#count sequence lengths and how often they appear\n",
        "freqMap = {}\n",
        "for d in data:\n",
        "  if d.shape not in freqMap.keys():\n",
        "    freqMap[d.shape] = 1\n",
        "\n",
        "  else:\n",
        "    freqMap[d.shape] += 1\n",
        "\n",
        "for key, value in freqMap.items():\n",
        "    print(f'{key} -> {value}')\n",
        "\n",
        "\n",
        "#split data into training/validation sets using an 85/15 split\n",
        "data = np.asarray(data, dtype=\"object\")    ######################Not needed when all sequences are the same length\n",
        "np.random.shuffle(data)\n",
        "\n",
        "trainData, valData = np.split(data, [int(0.85*len(data))])\n",
        "\n",
        "print(len(trainData), len(valData))\n",
        "\n",
        "#remove the labels from the trainingData and validation data, also storing them as a list\n",
        "trainLabels = []\n",
        "valLabels = []\n",
        "\n",
        "for d in trainData:\n",
        "  label = d.pop('Label')\n",
        "  trainLabels.append(label)\n",
        "\n",
        "for d in valLabels:\n",
        "  label = d.pop('Label')\n",
        "  valLabels.append(label)"
      ],
      "metadata": {
        "id": "4Xy6b8Q6LoY3",
        "outputId": "59d8a3ef-81ad-4239-9ffe-3a8cebebe79e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(31, 4) -> 381\n",
            "(30, 4) -> 31\n",
            "(14, 4) -> 2\n",
            "(29, 4) -> 21\n",
            "(5, 4) -> 1\n",
            "(11, 4) -> 1\n",
            "(21, 4) -> 4\n",
            "(4, 4) -> 1\n",
            "(7, 4) -> 1\n",
            "(27, 4) -> 10\n",
            "(28, 4) -> 15\n",
            "(25, 4) -> 4\n",
            "(26, 4) -> 6\n",
            "(17, 4) -> 2\n",
            "(22, 4) -> 2\n",
            "(24, 4) -> 2\n",
            "(23, 4) -> 2\n",
            "(15, 4) -> 1\n",
            "413 74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data padding\n",
        "#inputs to GRU cells are in the form of [batch, timesteps, feature]\n",
        "#our sequences have multiple lengths, so we need to pad this data\n",
        "\n",
        "#currently just removing all dfs that do not have length == 31\n"
      ],
      "metadata": {
        "id": "ypAB10mGl136"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize data using only the training set\n",
        "#find the min/max of each column throughout all of the dataframes in the training dataset\n",
        "#save the values, then iterate through the training and validation dataset and use min-max normalization\n",
        "#df[column] = (df[column] - df[column].min()) / (df[column].max() - df[column].min())\n",
        "\n",
        "#norm values stores the min/max of each column, initialized to min = float('inf'), max = float('-inf')\n",
        "#[[Heart Rate Min, Heart Rate Max], [Respiratory Rate Min, Respiratory Rate Max], [O2 Level Min, O2 Level Max]]\n",
        "normValues = [[float('inf'), float('-inf')], [float('inf'), float('-inf')], [float('inf'), float('-inf')]]\n",
        "\n",
        "for df in trainData:\n",
        "  normValues[0][0] = min(normValues[0][0], df['Heart Rate'].min())\n",
        "  normValues[0][1] = max(normValues[0][1], df['Heart Rate'].max())\n",
        "\n",
        "  normValues[1][0] = min(normValues[1][0], df['Respiratory Rate'].min())\n",
        "  normValues[1][1] = max(normValues[1][1], df['Respiratory Rate'].max())\n",
        "\n",
        "  normValues[2][0] = min(normValues[2][0], df['O2 Level'].min())\n",
        "  normValues[2][1] = max(normValues[2][1], df['O2 Level'].max())\n",
        "\n",
        "#now we have the min/max for each column across our whole dataset from our training data, so we can save this normalization, and apply it to the validation set\n",
        "print(normValues)\n",
        "\n",
        "\n",
        "\n",
        "#normalizing the data:\n",
        "for df in trainData:\n",
        "  df['Heart Rate'] = (df['Heart Rate'] - normValues[0][0]) / (normValues[0][1] - normValues[0][0])\n",
        "  df['Respiratory Rate'] = (df['Respiratory Rate'] - normValues[1][0]) / (normValues[1][1] - normValues[1][0])\n",
        "  df['O2 Level'] = (df['O2 Level'] - normValues[2][0]) / (normValues[2][1] - normValues[2][0])\n",
        "\n",
        "for df in valData:\n",
        "  df['Heart Rate'] = (df['Heart Rate'] - normValues[0][0]) / (normValues[0][1] - normValues[0][0])\n",
        "  df['Respiratory Rate'] = (df['Respiratory Rate'] - normValues[1][0]) / (normValues[1][1] - normValues[1][0])\n",
        "  df['O2 Level'] = (df['O2 Level'] - normValues[2][0]) / (normValues[2][1] - normValues[2][0])"
      ],
      "metadata": {
        "id": "FQgBFE7onqWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af99c290-b674-46d0-db7e-5c9f240608eb"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[44, 199], [15, 90], [80, 100]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Setup"
      ],
      "metadata": {
        "id": "upaZTSH3w0qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#first model uses 4 GRU layers with 16 unit each\n",
        "#input is taken in the form of: [batch, timesteps, feature]\n",
        "#Since our sequences have different lengths, we need to pad them\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(3),\n",
        "    tf.keras.layers.GRU(16, return_sequences=True),\n",
        "    tf.keras.layers.GRU(16, return_sequences=True),\n",
        "    tf.keras.layers.GRU(16, return_sequences=True),\n",
        "    tf.keras.layers.GRU(16),\n",
        "    tf.keras.layers.Dense(3)\n",
        "])"
      ],
      "metadata": {
        "id": "CClHL9t2w3kc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "e1f50828-13ff-4713-d2ac-6dedd98be1d5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input 0 of layer \"gru_12\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 3)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-4e1a01ae6aaf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model = tf.keras.models.Sequential([\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/trackable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    236\u001b[0m                     \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                     \u001b[0;34m\"is incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"gru_12\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 3)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "G1Fi-MTs5vzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#our learning rate scheduler. Will gradually shrink the learning rate as training progesses to find a better converging point.\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    0.001,\n",
        "    decay_steps=100000,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True)\n",
        "\n",
        "checkpoint_filepath = '/content/drive/MyDrive/Colab Notebooks/Deep Learning/Project 2/checkpoint'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule, epsilon=1e-7)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=opt,\n",
        "              loss_fn=loss,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "X0_nUOY_50Dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zlLqB23znTXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Script"
      ],
      "metadata": {
        "id": "AVq8bgK550VC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8M7ZIwNM53As"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}