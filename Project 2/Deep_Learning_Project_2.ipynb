{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFa/JwZ+iX/STj9e+0mHjH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChrisKantor/Deep-Learning/blob/main/Project%202/Deep_Learning_Project_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Deep learning model that builds on the one made in HW3 for determining an infants pain level using vital signs\n",
        "#This improved model uses LSTM to better understand the sequential readings\n",
        "\n",
        "\n",
        "\n",
        "#Run these commands - NECCESARY to use the GPU\n",
        "# export CUDNN_PATH=$(dirname $(python -c \"import nvidia.cudnn;print(nvidia.cudnn.__file__)\"))\n",
        "# export LD_LIBRARY_PATH=${CUDNN_PATH}/lib\n",
        "\n",
        "#To connect to local runtime: jupyter lab --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888 --NotebookApp.port_retries=0"
      ],
      "metadata": {
        "id": "-gD_j6_iqrxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aRuH-vPap5nA"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import PIL\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Some hyperparameter setup\n",
        "batch_size = 64           #set batch size to 64\n",
        "epochs = 20               #set num epochs to 20\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "# dir = 'Deep Learning/Project 2/'\n",
        "# data_dir = \"data/\"\n",
        "\n",
        "\n",
        "## Mount Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "data_dir = '/content/drive/MyDrive/Colab Notebooks/Deep Learning/Project 2/data/'\n",
        "\n",
        "\n",
        "import os\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z45cpkBIsMQ-",
        "outputId": "a63ffc6f-84e2-4374-99ec-dbd2848f836f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n",
            "Mounted at /content/drive\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "CrrDoReswut8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NEEDS FURTHER CLARIFICATION:\n",
        "#If a csv file sequence has an invalid row, do we just remove it or is the whole sequence invalid?\n",
        "#Can we mix the csv sequences together to get a fixed length input sequence? or should each seperate csv file be treated as its own sequence?\n",
        "#Are we predicting if the baby is in pain during the next timestep?\n",
        "\n",
        "\n",
        "\n",
        "#lots to do here - roughly follow this tutorial for general RNN processing: https://www.tensorflow.org/text/tutorials/text_generation\n",
        "\n",
        "#need sequences of data to use as input. Each csv file is 1 sequence, and each of them could potentially have missing/invalid data in them. 602 total csv files\n",
        "#Each csv file has 30 entries, but due to missing/invalid data, our cleaned sequences could have any number of entries\n",
        "#After cleaning the csv files, our dataset will be compromised of a bunch of variable length sequences\n",
        "#we will split our dataset up into training/validation sets on a sequence level\n",
        "\n",
        "#the inputs at a specific timestep will be the 3 labels in the csv files (Heart Rate, Respiratory Rate, O2 level) and the output will be (either if the baby is in pain OR if the baby is in pain during the NEXT step)? -Need clarification here"
      ],
      "metadata": {
        "id": "5wsgp_xGwyhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#store a list of dataframes representing the data from the csv files we read in\n",
        "#after reading in all of the csv files as dataframes, iterate through them, cleaning up the data and normalizing the inputs\n",
        "\n",
        "\n",
        "#Should we normalize on a dataframe level or a dataset level?\n",
        "\n",
        "\n",
        "data = []\n",
        "\n",
        "\n",
        "#use the os library to iterate through all files in the current path\n",
        "for (root,dirs,files) in os.walk(data_dir, topdown=True):\n",
        "  #open each CSV file, we don't want to open any other type of file\n",
        "  for f in files:\n",
        "    if f.endswith(\".csv\"):\n",
        "      with open(os.path.join(root, f), mode ='r') as csvFile:\n",
        "\n",
        "        #read in the csv file\n",
        "        csvData = csv.reader(csvFile)\n",
        "\n",
        "        #store the data temporarily before we turn it into a pandas dataframe\n",
        "        temp = []\n",
        "\n",
        "\n",
        "        #check if each row is valid\n",
        "        #one way to tell if the data is invalid is if the label is NOT [0, 1, 2]. This means there was something wrong with one of the sensors at the time of capture\n",
        "        #another way is to check if the heart rate, respiratory rate, or o2 level is too low or too high\n",
        "        #for heart Rate, we want a range of 40 - 200\n",
        "        #for Respiratory Rate, we want a range of 15 - 90\n",
        "        #for o2 level, we want a rate of 80 - 100\n",
        "\n",
        "        for row in csvData:\n",
        "          try:\n",
        "            if row[4] in \"012\" and (int(row[1]) >= 40 and int(row[1]) <= 200) and (int(row[2]) >= 15 and int(row[2]) <= 90) and (int(row[3]) >= 80 and int(row[3]) <= 100):   #data is valid, so add it to the main csv\n",
        "              #row[0] just stores the row # in the csv file, so it is not needed in the training dataset\n",
        "              temp.append([int(row[1]), int(row[2]), int(row[3]), int(row[4])])\n",
        "\n",
        "          #Catches errors that occur when an element cannot be parsed into an integer. In this case the row is invalid so we can skip it\n",
        "          except ValueError:\n",
        "            continue\n",
        "\n",
        "        #if this valid and cleaned sequence is NOT empty, add it to our dataset\n",
        "        if len(temp) > 0:\n",
        "          df = pd.DataFrame(temp, columns = ['Heart Rate', 'Respiratory Rate', 'O2 Level', 'Label'])\n",
        "          data.append(df)\n",
        "\n",
        "print(len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0brwvTJ5rkZ",
        "outputId": "0c3eb042-6360-40e2-a13e-375a56f846b7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#count sequence lengths and how often they appear\n",
        "freqMap = {}\n",
        "for d in data:\n",
        "  if d.shape not in freqMap.keys():\n",
        "    freqMap[d.shape] = 1\n",
        "\n",
        "  else:\n",
        "    freqMap[d.shape] += 1\n",
        "\n",
        "for key, value in freqMap.items():\n",
        "    print(f'{key} -> {value}')\n",
        "\n",
        "\n",
        "#split data into training/validation sets using an 85/15 split\n",
        "data = np.asarray(data, dtype=\"object\")\n",
        "np.random.shuffle(data)\n",
        "\n",
        "trainData, valData = np.split(data, [int(0.85*len(data))])\n",
        "\n",
        "print(len(trainData), len(valData))\n",
        "\n",
        "#remove the labels from the trainingData and validation data, also storing them as a list\n",
        "#we don't need the entire column of labels, just the first rows label\n",
        "trainLabels = []\n",
        "valLabels = []\n",
        "\n",
        "for df in trainData:\n",
        "  label = df['Label'].iloc[0]    #get the first rows label, and use it for this whole sequence\n",
        "  df.drop('Label', axis=1, inplace=True)\n",
        "  trainLabels.append(label)\n",
        "\n",
        "for df in valData:\n",
        "  label = df['Label'].iloc[0]    #get the first rows label, and use it for this whole sequence\n",
        "  df.drop('Label', axis=1, inplace=True)\n",
        "  valLabels.append(label)\n",
        "\n",
        "print(trainData.shape, len(trainLabels))\n",
        "print(valData.shape, len(valLabels))"
      ],
      "metadata": {
        "id": "4Xy6b8Q6LoY3",
        "outputId": "29092e6a-6383-427e-f4d6-4ff533b1644e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(31, 4) -> 381\n",
            "(30, 4) -> 31\n",
            "(14, 4) -> 2\n",
            "(29, 4) -> 21\n",
            "(5, 4) -> 1\n",
            "(11, 4) -> 1\n",
            "(21, 4) -> 4\n",
            "(4, 4) -> 1\n",
            "(7, 4) -> 1\n",
            "(27, 4) -> 10\n",
            "(28, 4) -> 15\n",
            "(25, 4) -> 4\n",
            "(26, 4) -> 6\n",
            "(17, 4) -> 2\n",
            "(22, 4) -> 2\n",
            "(24, 4) -> 2\n",
            "(23, 4) -> 2\n",
            "(15, 4) -> 1\n",
            "413 74\n",
            "(413,) 413\n",
            "(74,) 74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize data using only the training set\n",
        "#find the min/max of each column throughout all of the dataframes in the training dataset\n",
        "#save the values, then iterate through the training and validation dataset and use min-max normalization\n",
        "#df[column] = (df[column] - df[column].min()) / (df[column].max() - df[column].min())\n",
        "\n",
        "#norm values stores the min/max of each column, initialized to min = float('inf'), max = float('-inf')\n",
        "#[[Heart Rate Min, Heart Rate Max], [Respiratory Rate Min, Respiratory Rate Max], [O2 Level Min, O2 Level Max]]\n",
        "normValues = [[float('inf'), float('-inf')], [float('inf'), float('-inf')], [float('inf'), float('-inf')]]\n",
        "\n",
        "for df in trainData:\n",
        "  normValues[0][0] = min(normValues[0][0], df['Heart Rate'].min())\n",
        "  normValues[0][1] = max(normValues[0][1], df['Heart Rate'].max())\n",
        "\n",
        "  normValues[1][0] = min(normValues[1][0], df['Respiratory Rate'].min())\n",
        "  normValues[1][1] = max(normValues[1][1], df['Respiratory Rate'].max())\n",
        "\n",
        "  normValues[2][0] = min(normValues[2][0], df['O2 Level'].min())\n",
        "  normValues[2][1] = max(normValues[2][1], df['O2 Level'].max())\n",
        "\n",
        "#now we have the min/max for each column across our whole dataset from our training data, so we can save this normalization, and apply it to the validation set\n",
        "print(normValues)\n",
        "\n",
        "\n",
        "\n",
        "#normalizing the data:\n",
        "for df in trainData:\n",
        "  df['Heart Rate'] = (df['Heart Rate'] - normValues[0][0]) / (normValues[0][1] - normValues[0][0])\n",
        "  df['Respiratory Rate'] = (df['Respiratory Rate'] - normValues[1][0]) / (normValues[1][1] - normValues[1][0])\n",
        "  df['O2 Level'] = (df['O2 Level'] - normValues[2][0]) / (normValues[2][1] - normValues[2][0])\n",
        "\n",
        "for df in valData:\n",
        "  df['Heart Rate'] = (df['Heart Rate'] - normValues[0][0]) / (normValues[0][1] - normValues[0][0])\n",
        "  df['Respiratory Rate'] = (df['Respiratory Rate'] - normValues[1][0]) / (normValues[1][1] - normValues[1][0])\n",
        "  df['O2 Level'] = (df['O2 Level'] - normValues[2][0]) / (normValues[2][1] - normValues[2][0])"
      ],
      "metadata": {
        "id": "FQgBFE7onqWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c897a5af-ed59-4c94-ecf2-6f3464deb046"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[52, 199], [15, 90], [80, 100]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data padding\n",
        "#inputs to GRU cells are in the form of [batch, timesteps, feature]\n",
        "#our sequences have multiple lengths, so we need to pad this data to get all of the sequences to have a length of 31 (indices 0-30)\n",
        "#we can pad the data by adding rows of 0's, and use a masking layer to ignore them when training the model\n",
        "\n",
        "for i in range(len(trainData)):\n",
        "  if trainData[i].shape[0] < 31:\n",
        "    padding = pd.DataFrame(0, index=np.arange(trainData[i].shape[0], 31), columns=trainData[i].columns)\n",
        "    trainData[i] = pd.concat([trainData[i], padding])\n",
        "\n",
        "for i in range(len(valData)):\n",
        "  if valData[i].shape[0] < 31:\n",
        "    padding = pd.DataFrame(0, index=np.arange(valData[i].shape[0], 31), columns=valData[i].columns)\n",
        "    valData[i] = pd.concat([valData[i], padding])"
      ],
      "metadata": {
        "id": "ypAB10mGl136"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Setup"
      ],
      "metadata": {
        "id": "upaZTSH3w0qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#first model uses 4 GRU layers with 16 unit each\n",
        "#input is taken in the form of: [batch, timesteps, feature]\n",
        "#Since our sequences have different lengths, we need to pad them\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(3),\n",
        "    tf.keras.layers.GRU(16, return_sequences=True),\n",
        "    tf.keras.layers.GRU(16, return_sequences=True),\n",
        "    tf.keras.layers.GRU(16, return_sequences=True),\n",
        "    tf.keras.layers.GRU(16),\n",
        "    tf.keras.layers.Dense(3)\n",
        "])"
      ],
      "metadata": {
        "id": "CClHL9t2w3kc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "3899210b-fc02-4d95-cf0f-ffdd2d365269"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input 0 of layer \"gru\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 3)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-d2fe391f8372>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#input is taken in the form of: [batch, timesteps, feature]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Since our sequences have different lengths, we need to pad them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m model = tf.keras.models.Sequential([\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/trackable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    236\u001b[0m                     \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                     \u001b[0;34m\"is incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"gru\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 3)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "G1Fi-MTs5vzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#our learning rate scheduler. Will gradually shrink the learning rate as training progesses to find a better converging point.\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    0.001,\n",
        "    decay_steps=100000,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True)\n",
        "\n",
        "checkpoint_filepath = '/content/drive/MyDrive/Colab Notebooks/Deep Learning/Project 2/checkpoint'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule, epsilon=1e-7)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=opt,\n",
        "              loss_fn=loss,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "X0_nUOY_50Dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zlLqB23znTXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Script"
      ],
      "metadata": {
        "id": "AVq8bgK550VC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8M7ZIwNM53As"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}