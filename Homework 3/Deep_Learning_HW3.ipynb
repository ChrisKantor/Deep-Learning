{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhgkX/vi/cZa34jJCWbwh/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChrisKantor/Deep-Learning/blob/main/Homework%203/Deep_Learning_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCY4N1ruqq5u"
      },
      "outputs": [],
      "source": [
        "#Deep Learning Assignment Notes:\n",
        "#Need to submite 3 things, training code, the trained model, and then a bunch of results.\n",
        "#need to try all of the things listed in q2\n",
        "#lots of tial and eror with the hyperparameter tuning\n",
        "\n",
        "#data set is about baby vitals for detecting baby pain\n",
        "#3 classes: no pain, mild pain, lots of pain\n",
        "#col 1 is heart rate\n",
        "#col 2 is respiratory rate\n",
        "#col 3 is oxygen saturation\n",
        "#Can merge all the csv files into 1 large one\n",
        "#randomize and then split into training/validation data\n",
        "\n",
        "#need to clean the data, invalid data could be in any column\n",
        "\n",
        "#need 5 HIDDEN layers, so 7 total with the input and output layers\n",
        "#We can choose any number of neurons per layer\n",
        "\n",
        "#70% accuracy is a good minimum\n",
        "\n",
        "#Submit the best model only, but save figures for the other models with hyperparameter tuning\n",
        "#the model is not enough, need to submit a script which loads the model and then runs the test dataset on it\n",
        "#the test data has 2 folders same format as training data, need to combine the data into 1 csv\n",
        "\n",
        "#the testing script can be on a seperate cell in the notebook\n",
        "\n",
        "#figures are for validation loss\n",
        "#don't need to combine hyperparameters, just do them without each other, should be 12 figures total for part 2\n",
        "#1 figure for part 1 (the best model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading Dataset**"
      ],
      "metadata": {
        "id": "qC0rRcFpQf42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import os\n",
        "\n",
        "\n",
        "## Mount Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "data_dir = '/content/drive/MyDrive/Colab Notebooks/Deep Learning/Homework 3 Data/'\n",
        "\n",
        "\n",
        "mainFile = \"baby_data.csv\"\n",
        "\n",
        "#create a new csv file that will hold all of our valid data\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/Deep Learning/Homework 3 Data/\" + mainFile, 'w') as csvfile:\n",
        "  csvwriter = csv.writer(csvfile)\n",
        "\n",
        "  # writing the column names\n",
        "  csvwriter.writerow(['Heart Rate', 'Respiratory Rate', 'O2 Level', 'Label'])\n",
        "\n",
        "  #use the os library to iterate through all files in the current path\n",
        "  for (root,dirs,files) in os.walk(data_dir, topdown=True):\n",
        "    #open each CSV file, we don't want to open any other type of file\n",
        "    for f in files:\n",
        "      if f.endswith(\".csv\"):\n",
        "        #in each file, go through each row and check if it is valid\n",
        "        with open(os.path.join(root, f), mode ='r') as csvFile:\n",
        "          csvData = csv.reader(csvFile)\n",
        "\n",
        "          #one way to tell if the data is invalid is if the label is NOT [0, 1, 2]. This means there was something wrong with one of the sensors at the time of capture\n",
        "          #another way is to check if the heart rate, respiratory rate, or o2 level is too low, in this case we check if it is <= 0. if so, there was something wrong with the sensor\n",
        "          for row in csvData:\n",
        "            try:\n",
        "              if row[4] in \"012\" and int(row[1]) > 0 and int(row[2]) > 0 and int(row[3]) > 0:   #data is valid, so add it to the main csv\n",
        "\n",
        "                #row[0] just stores the row # in the csv file, so it is not needed in the training dataset\n",
        "                csvwriter.writerow([row[1], row[2], row[3], row[4]])\n",
        "\n",
        "            #Catches errors that occur when an element cannot be parsed into an integer. In this case the row is invalid so we can skip it\n",
        "            except ValueError:\n",
        "              continue\n",
        "\n",
        "print(\"Done writing to main csv file: \", mainFile)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWP_e1NDQorH",
        "outputId": "01c8490b-3ba9-4b53-ef8a-1365ad09c33a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Done writing to main csv file:  baby_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This function turns the pandas dataframe into a tensorflow dataset to use for training\n",
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  df = dataframe.copy()\n",
        "  labels = df.pop('Label')\n",
        "  df = {key: value.values[:,tf.newaxis] for key, value in dataframe.items()}\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(batch_size)\n",
        "  return ds\n",
        "\n",
        "\n",
        "#This function normalizes NUMERICAL data\n",
        "def get_normalization_layer(name, dataset):\n",
        "  # Create a Normalization layer for the feature.\n",
        "  normalizer = tf.keras.layers.Normalization(axis=None)\n",
        "\n",
        "  # Prepare a Dataset that only yields the feature.\n",
        "  feature_ds = dataset.map(lambda x, y: x[name])\n",
        "\n",
        "  # Learn the statistics of the data.\n",
        "  normalizer.adapt(feature_ds)\n",
        "\n",
        "  return normalizer"
      ],
      "metadata": {
        "id": "G6cnVUTKscGS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we have a csv file that has ALL of the valid data which we will use in training our model\n",
        "#The next step is to split the data into a training and validation set, using a 90/10 split\n",
        "dataframe = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Deep Learning/Homework 3 Data/\" + mainFile, sep=',')\n",
        "print(dataframe.shape)\n",
        "print(dataframe.head())\n",
        "\n",
        "#split the data into a training and validation set using a 90/10 split\n",
        "train, val = np.split(dataframe.sample(frac=1), [int(0.9*len(dataframe))])\n",
        "\n",
        "print(len(train), 'training examples')\n",
        "print(len(val), 'validation examples')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#A hyperparameter to tune, batch size\n",
        "batch_size = 32\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "\n",
        "\n",
        "all_inputs = []\n",
        "encoded_features = []\n",
        "\n",
        "# Numerical features.\n",
        "for header in ['Heart Rate', 'Respiratory Rate', 'O2 Level']:\n",
        "  numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
        "  normalization_layer = get_normalization_layer(header, train_ds)\n",
        "  encoded_numeric_col = normalization_layer(numeric_col)\n",
        "  all_inputs.append(numeric_col)\n",
        "  encoded_features.append(encoded_numeric_col)\n",
        "\n",
        "#merge all the features into one vector\n",
        "all_features = tf.keras.layers.concatenate(encoded_features)\n",
        "\n",
        "print(\"All Features:\\n\", all_features)"
      ],
      "metadata": {
        "id": "oMnxFybjZPoQ",
        "outputId": "49ee054a-c448-45ad-f666-76124df32fe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16531, 4)\n",
            "   Heart Rate  Respiratory Rate  O2 Level  Label\n",
            "0         172                46        98      2\n",
            "1         172                46        98      2\n",
            "2         173                46        98      2\n",
            "3         175                48        98      2\n",
            "4         172                49        98      2\n",
            "14877 training examples\n",
            "1654 validation examples\n",
            "All Features:\n",
            " KerasTensor(type_spec=TensorSpec(shape=(None, 3), dtype=tf.float32, name=None), name='concatenate_2/concat:0', description=\"created by layer 'concatenate_2'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Creation and Training"
      ],
      "metadata": {
        "id": "rSBe9Q_gb8W4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the model\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(3, activation='relu'),\n",
        "  tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(3)\n",
        "])\n",
        "\n",
        "#creating the loss function we will use\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "-CdMRySJcCgc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running the model\n",
        "model.fit(train_ds, epochs=10, validation_data=val_ds)\n",
        "\n",
        "loss, accuracy = model.evaluate(val_ds)\n",
        "print(\"Accuracy\", accuracy)"
      ],
      "metadata": {
        "id": "3TMqqRZZflHh",
        "outputId": "077c7f9b-33c4-486c-942c-4bd165969c92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential' (type Sequential).\n    \n    Layer \"dense\" expects 1 input(s), but it received 4 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 1) dtype=int64>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 1) dtype=int64>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 1) dtype=int64>, <tf.Tensor 'IteratorGetNext:3' shape=(None, 1) dtype=int64>]\n    \n    Call arguments received by layer 'sequential' (type Sequential):\n      • inputs={'Heart Rate': 'tf.Tensor(shape=(None, 1), dtype=int64)', 'Respiratory Rate': 'tf.Tensor(shape=(None, 1), dtype=int64)', 'O2 Level': 'tf.Tensor(shape=(None, 1), dtype=int64)', 'Label': 'tf.Tensor(shape=(None, 1), dtype=int64)'}\n      • training=True\n      • mask=None\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e598caec3a5d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Running the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential' (type Sequential).\n    \n    Layer \"dense\" expects 1 input(s), but it received 4 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 1) dtype=int64>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 1) dtype=int64>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 1) dtype=int64>, <tf.Tensor 'IteratorGetNext:3' shape=(None, 1) dtype=int64>]\n    \n    Call arguments received by layer 'sequential' (type Sequential):\n      • inputs={'Heart Rate': 'tf.Tensor(shape=(None, 1), dtype=int64)', 'Respiratory Rate': 'tf.Tensor(shape=(None, 1), dtype=int64)', 'O2 Level': 'tf.Tensor(shape=(None, 1), dtype=int64)', 'Label': 'tf.Tensor(shape=(None, 1), dtype=int64)'}\n      • training=True\n      • mask=None\n"
          ]
        }
      ]
    }
  ]
}