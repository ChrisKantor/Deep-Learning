{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmUhLxjHfX2wZbMUJ18rWw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChrisKantor/Deep-Learning/blob/main/Homework%203/Deep_Learning_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCY4N1ruqq5u"
      },
      "outputs": [],
      "source": [
        "#Deep Learning Assignment Notes:\n",
        "#Need to submite 3 things, training code, the trained model, and then a bunch of results.\n",
        "#need to try all of the things listed in q2\n",
        "#lots of tial and eror with the hyperparameter tuning\n",
        "\n",
        "#data set is about baby vitals for detecting baby pain\n",
        "#3 classes: no pain, mild pain, lots of pain\n",
        "#col 1 is heart rate\n",
        "#col 2 is respiratory rate\n",
        "#col 3 is oxygen saturation\n",
        "#Can merge all the csv files into 1 large one\n",
        "#randomize and then split into training/validation data\n",
        "\n",
        "#need to clean the data, invalid data could be in any column\n",
        "\n",
        "#need 5 HIDDEN layers, so 7 total with the input and output layers\n",
        "#We can choose any number of neurons per layer\n",
        "\n",
        "#70% accuracy is a good minimum\n",
        "\n",
        "#Submit the best model only, but save figures for the other models with hyperparameter tuning\n",
        "#the model is not enough, need to submit a script which loads the model and then runs the test dataset on it\n",
        "#the test data has 2 folders same format as training data, need to combine the data into 1 csv\n",
        "\n",
        "#the testing script can be on a seperate cell in the notebook\n",
        "\n",
        "#figures are for validation loss\n",
        "#don't need to combine hyperparameters, just do them without each other, should be 12 figures total for part 2\n",
        "#1 figure for part 1 (the best model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading Dataset**"
      ],
      "metadata": {
        "id": "qC0rRcFpQf42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import os\n"
      ],
      "metadata": {
        "id": "occsCpZzfvaK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Mount Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive/Colab Notebooks/Deep Learning/Homework 3 Data/'\n",
        "\n",
        "\n",
        "mainFile = \"baby_data.csv\"\n",
        "\n",
        "#create a new csv file that will hold all of our valid data\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/Deep Learning/Homework 3 Data/\" + mainFile, 'w') as csvfile:\n",
        "  csvwriter = csv.writer(csvfile)\n",
        "\n",
        "  # writing the column names\n",
        "  csvwriter.writerow(['Heart Rate', 'Respiratory Rate', 'O2 Level', 'Label'])\n",
        "\n",
        "  #use the os library to iterate through all files in the current path\n",
        "  for (root,dirs,files) in os.walk(data_dir, topdown=True):\n",
        "    #open each CSV file, we don't want to open any other type of file\n",
        "    for f in files:\n",
        "      if f.endswith(\".csv\"):\n",
        "        #in each file, go through each row and check if it is valid\n",
        "        with open(os.path.join(root, f), mode ='r') as csvFile:\n",
        "          csvData = csv.reader(csvFile)\n",
        "\n",
        "          #one way to tell if the data is invalid is if the label is NOT [0, 1, 2]. This means there was something wrong with one of the sensors at the time of capture\n",
        "          #another way is to check if the heart rate, respiratory rate, or o2 level is too low, in this case we check if it is <= 0. if so, there was something wrong with the sensor\n",
        "          for row in csvData:\n",
        "            try:\n",
        "              if row[4] in \"012\" and int(row[1]) > 0 and int(row[2]) > 0 and int(row[3]) > 0:   #data is valid, so add it to the main csv\n",
        "\n",
        "                #row[0] just stores the row # in the csv file, so it is not needed in the training dataset\n",
        "                csvwriter.writerow([row[1], row[2], row[3], row[4]])\n",
        "\n",
        "            #Catches errors that occur when an element cannot be parsed into an integer. In this case the row is invalid so we can skip it\n",
        "            except ValueError:\n",
        "              continue\n",
        "\n",
        "print(\"Done writing to main csv file: \", mainFile)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWP_e1NDQorH",
        "outputId": "9ec3a7d1-f2af-4ed0-aad1-d61ac56fcfa3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Done writing to main csv file:  baby_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This function turns the pandas dataframe into a tensorflow dataset to use for training\n",
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  df = dataframe.copy()\n",
        "  labels = df.pop('Label')\n",
        "  df = {key: value.values[:,tf.newaxis] for key, value in dataframe.items()}\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(batch_size)\n",
        "  return ds\n",
        "\n",
        "\n",
        "#This function normalizes NUMERICAL data\n",
        "def get_normalization_layer(name, dataset):\n",
        "  # Create a Normalization layer for the feature.\n",
        "  normalizer = tf.keras.layers.Normalization(axis=None)\n",
        "\n",
        "  # Prepare a Dataset that only yields the feature.\n",
        "  feature_ds = dataset.map(lambda x, y: x[name])\n",
        "\n",
        "  # Learn the statistics of the data.\n",
        "  normalizer.adapt(feature_ds)\n",
        "\n",
        "  return normalizer"
      ],
      "metadata": {
        "id": "G6cnVUTKscGS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we have a csv file that has ALL of the valid data which we will use in training our model\n",
        "#The next step is to split the data into a training and validation set, using a 90/10 split\n",
        "dataframe = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Deep Learning/Homework 3 Data/\" + mainFile, sep=',')\n",
        "print(dataframe.shape)\n",
        "print(dataframe.head())\n",
        "\n",
        "\n",
        "#split the data into a training and validation set using a 90/10 split, and randomize beforehand\n",
        "trainData, valData = np.split(dataframe.sample(frac=1), [int(0.9*len(dataframe))])\n",
        "\n",
        "#save the labels to a seperate dataframe and remove them from the data dataframe\n",
        "trainLabel = trainData.pop('Label')\n",
        "valLabel = valData.pop('Label')\n",
        "\n",
        "\n",
        "# #normalize the data using pandas\n",
        "# for col in trainData.columns:\n",
        "#   trainData[col] = trainData[col]  / trainData[col].abs().max()\n",
        "\n",
        "# for col in valData.columns:\n",
        "#   valData[col] = valData[col]  / valData[col].abs().max()\n",
        "\n",
        "\n",
        "# #A hyperparameter to tune, batch size\n",
        "# batch_size = 32\n",
        "# train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "# val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "\n",
        "\n",
        "# all_inputs = []\n",
        "# encoded_features = []\n",
        "\n",
        "# # Numerical features.\n",
        "# for header in ['Heart Rate', 'Respiratory Rate', 'O2 Level']:\n",
        "#   numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
        "#   normalization_layer = get_normalization_layer(header, train_ds)\n",
        "#   encoded_numeric_col = normalization_layer(numeric_col)\n",
        "#   all_inputs.append(numeric_col)\n",
        "#   encoded_features.append(encoded_numeric_col)\n",
        "\n",
        "# #merge all the features into one vector\n",
        "# all_features = tf.keras.layers.concatenate(encoded_features)\n",
        "\n",
        "# print(\"All Features:\\n\", all_features)"
      ],
      "metadata": {
        "id": "oMnxFybjZPoQ",
        "outputId": "599ff4c5-c767-4d43-fcab-cf6460cb5d66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16531, 4)\n",
            "   Heart Rate  Respiratory Rate  O2 Level  Label\n",
            "0         172                46        98      2\n",
            "1         172                46        98      2\n",
            "2         173                46        98      2\n",
            "3         175                48        98      2\n",
            "4         172                49        98      2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Creation and Training"
      ],
      "metadata": {
        "id": "rSBe9Q_gb8W4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the model\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(3),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(3)\n",
        "])\n",
        "\n",
        "#creating the loss function we will use\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model.compile(optimizer='Adam',\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "-CdMRySJcCgc"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running the model\n",
        "print(\"Running Model\\n\")\n",
        "model.fit(trainData, trainLabel, epochs=10, batch_size=32)\n",
        "\n",
        "print(\"\\n\\nEvaluating Model\\n\")\n",
        "#evaluation the model on the validation data\n",
        "model.evaluate(valData, valLabel, verbose=2)\n",
        "\n",
        "#save the model\n",
        "model.save(\"/content/drive/MyDrive/Colab Notebooks/Deep Learning/Homework 3 Data/baby_pain_model.keras\")\n",
        "print(\"Saved Model\")"
      ],
      "metadata": {
        "id": "3TMqqRZZflHh",
        "outputId": "e53d2415-f50d-4e1b-82cc-6c6cc8cd1d28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Model\n",
            "\n",
            "Epoch 1/10\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.6556 - accuracy: 0.6991\n",
            "Epoch 2/10\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6978\n",
            "Epoch 3/10\n",
            "465/465 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.7021\n",
            "Epoch 4/10\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.6550 - accuracy: 0.7000\n",
            "Epoch 5/10\n",
            "465/465 [==============================] - 2s 3ms/step - loss: 0.6603 - accuracy: 0.7007\n",
            "Epoch 6/10\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.6526 - accuracy: 0.7011\n",
            "Epoch 7/10\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.7044\n",
            "Epoch 8/10\n",
            "465/465 [==============================] - 3s 6ms/step - loss: 0.6492 - accuracy: 0.7058\n",
            "Epoch 9/10\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.6492 - accuracy: 0.7038\n",
            "Epoch 10/10\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.6495 - accuracy: 0.7038\n",
            "\n",
            "\n",
            "Evaluating Model\n",
            "\n",
            "52/52 - 0s - loss: 0.6640 - accuracy: 0.6971 - 100ms/epoch - 2ms/step\n",
            "Saved Model\n"
          ]
        }
      ]
    }
  ]
}