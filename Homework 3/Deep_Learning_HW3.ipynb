{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOe1bvO2yzw7MlYa3n7pzil",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChrisKantor/Deep-Learning/blob/main/Homework%203/Deep_Learning_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCY4N1ruqq5u"
      },
      "outputs": [],
      "source": [
        "#Deep Learning Assignment Notes:\n",
        "#Need to submite 3 things, training code, the trained model, and then a bunch of results.\n",
        "#need to try all of the things listed in q2\n",
        "#lots of tial and eror with the hyperparameter tuning\n",
        "\n",
        "#data set is about baby vitals for detecting baby pain\n",
        "#3 classes: no pain, mild pain, lots of pain\n",
        "#col 1 is heart rate\n",
        "#col 2 is respiratory rate\n",
        "#col 3 is oxygen saturation\n",
        "#Can merge all the csv files into 1 large one\n",
        "#randomize and then split into training/validation data\n",
        "\n",
        "#need to clean the data, invalid data could be in any column\n",
        "\n",
        "#need 5 HIDDEN layers, so 7 total with the input and output layers\n",
        "#We can choose any number of neurons per layer\n",
        "\n",
        "#70% accuracy is a good minimum\n",
        "\n",
        "#Submit the best model only, but save figures for the other models with hyperparameter tuning\n",
        "#the model is not enough, need to submit a script which loads the model and then runs the test dataset on it\n",
        "#the test data has 2 folders same format as training data, need to combine the data into 1 csv\n",
        "\n",
        "#the testing script can be on a seperate cell in the notebook\n",
        "\n",
        "#figures are for validation loss\n",
        "#don't need to combine hyperparameters, just do them without each other, should be 12 figures total for part 2\n",
        "#1 figure for part 1 (the best model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading Dataset**"
      ],
      "metadata": {
        "id": "qC0rRcFpQf42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import os\n",
        "\n",
        "\n",
        "## Mount Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "data_dir = '/content/drive/MyDrive/Colab Notebooks/Deep Learning/Homework 3 Data/'\n",
        "\n",
        "\n",
        "mainFile = \"baby_data.csv\"\n",
        "\n",
        "#create a new csv file that will hold all of our valid data\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/Deep Learning/Homework 3 Data/\" + mainFile, 'w') as csvfile:\n",
        "  csvwriter = csv.writer(csvfile)\n",
        "\n",
        "  # writing the column names\n",
        "  csvwriter.writerow(['Heart Rate', 'Respiratory Rate', 'O2 Level', 'Label'])\n",
        "\n",
        "  #use the os library to iterate through all files in the current path\n",
        "  for (root,dirs,files) in os.walk(data_dir, topdown=True):\n",
        "    #open each CSV file, we don't want to open any other type of file\n",
        "    for f in files:\n",
        "      if f.endswith(\".csv\"):\n",
        "        #in each file, go through each row and check if it is valid\n",
        "        with open(os.path.join(root, f), mode ='r') as csvFile:\n",
        "          csvData = csv.reader(csvFile)\n",
        "\n",
        "          #one way to tell if the data is invalid is if the label is NOT [0, 1, 2]. This means there was something wrong with one of the sensors at the time of capture\n",
        "          #another way is to check if the heart rate, respiratory rate, or o2 level is too low, in this case we check if it is <= 0. if so, there was something wrong with the sensor\n",
        "          for row in csvData:\n",
        "            try:\n",
        "              if row[4] in \"012\" and int(row[1]) > 0 and int(row[2]) > 0 and int(row[3]) > 0:   #data is valid, so add it to the main csv\n",
        "\n",
        "                #row[0] just stores the row # in the csv file, so it is not needed in the training dataset\n",
        "                csvwriter.writerow([row[1], row[2], row[3], row[4]])\n",
        "\n",
        "            #Catches errors that occur when an element cannot be parsed into an integer. In this case the row is invalid so we can skip it\n",
        "            except ValueError:\n",
        "              continue\n",
        "\n",
        "print(\"Done writing to main csv file: \", mainFile)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWP_e1NDQorH",
        "outputId": "b767cca0-ce48-4309-e00d-4fd338b5cc73"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Done writing to main csv file:  baby_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we have a csv file that has ALL of the valid data which we will use in training our model\n",
        "#The next step is to split the data into a training and validation set, using a 90/10 split\n",
        "dataframe = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Deep Learning/Homework 3 Data/\" + mainFile, sep=',')\n",
        "print(dataframe.shape)\n",
        "\n",
        "#split the data into a training and validation set using a 90/10 split\n",
        "train, val = np.split(dataframe.sample(frac=1), [int(0.9*len(dataframe))])\n",
        "\n",
        "print(len(train), 'training examples')\n",
        "print(len(val), 'validation examples')\n",
        "\n",
        "\n",
        "#turn the pandas dataframe into a tensorflow dataset to use for training\n",
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  df = dataframe.copy()\n",
        "  labels = df.pop('Label')\n",
        "  df = {key: value.values[:,tf.newaxis] for key, value in dataframe.items()}\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(batch_size)\n",
        "  return ds\n",
        "\n",
        "\n",
        "#A hyperparameter to tune, batch size\n",
        "batch_size = 32\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6cnVUTKscGS",
        "outputId": "e8255a1a-1886-4894-bbe7-0801c7af21ef"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16531, 4)\n",
            "14877 training examples\n",
            "1654 validation examples\n"
          ]
        }
      ]
    }
  ]
}